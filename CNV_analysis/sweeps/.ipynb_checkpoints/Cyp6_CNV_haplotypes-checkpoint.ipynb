{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haplotype analysis *Cyp6*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color palettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionary of pop colors\n",
    "pop_colours = {\n",
    " 'Moshi' : \"lightgreen\",\n",
    " 'Muleba' : \"darkgreen\",\n",
    "}\n",
    "\n",
    "dup_colours = {\n",
    "    'Dup31 present' : 'mediumorchid',\n",
    "    'Dup31 absent' : 'lightgray',\n",
    "    'Dup33 present' : 'orchid',\n",
    "    'Dup33 absent' : 'lightgray',\n",
    "    '2 extra copies': 'purple',\n",
    "    '1 extra copy': 'orange',\n",
    "    'No CNV': 'lightgray'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import malariagen_data\n",
    "import allel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import scipy.cluster\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CNV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify samples that have Cyp6_Dup31\n",
    "disc_cnv_fn = '../Ag1000G_CNV_data/v3.7_1246-VO-TZ-KABULA-VMF00185/target_regions_analysis/focal_region_CNV_table.csv'\n",
    "discordant_read_calls = pd.read_csv(disc_cnv_fn, sep = '\\t', index_col = 0)\n",
    "has_Dup31 = discordant_read_calls['Cyp6aap_Dup31'].values\n",
    "Dup31_calls = has_Dup31.astype('int')\n",
    "has_Dup33 = discordant_read_calls['Cyp6aap_Dup33'].values\n",
    "Dup33_calls = has_Dup33.astype('int')\n",
    "\n",
    "# In order to known the copy number of each Dup specifically, we need to ignore samples that also\n",
    "# have another CNV at Cyp6. Dup32 is always found with Dup31, so we consider them to be the same. \n",
    "other_cyp6_cnvs = (\n",
    "    discordant_read_calls.columns.str.contains('Cyp6aap') &\n",
    "    ~discordant_read_calls.columns.str.contains('Dup0') &\n",
    "    ~discordant_read_calls.columns.str.contains('Dup31') &\n",
    "    ~discordant_read_calls.columns.str.contains('Dup32') &\n",
    "    ~discordant_read_calls.columns.str.contains('Dup33')\n",
    ")\n",
    "other_cyp6_calls = discordant_read_calls.loc[:, other_cyp6_cnvs]\n",
    "has_other_cyp6_cnv = np.sum(other_cyp6_calls, 1).values > 0\n",
    "\n",
    "# Now we can exclude samples that have both the focal and another CNV allele, or that have the \n",
    "# allele and are of high coverage variance (because we won't be able to assign copy number correctly)\n",
    "exclude_samples_Dup31 = (  \n",
    "    has_Dup31 & (\n",
    "        has_Dup33 |\n",
    "        has_other_cyp6_cnv | \n",
    "        discordant_read_calls[\"High.var.sample\"]\n",
    "    )                  \n",
    ").values\n",
    "exclude_samples_Dup33 = (  \n",
    "    has_Dup33 & (\n",
    "        has_Dup31 |\n",
    "        has_other_cyp6_cnv | \n",
    "        discordant_read_calls[\"High.var.sample\"]\n",
    "    )                  \n",
    ").values\n",
    "\n",
    "Dup33_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    const el = document.getElementById(null);\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.2.min.js\"];\n",
       "  const css_urls = [];\n",
       "  \n",
       "\n",
       "  const inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(null)).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(null);\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.2.min.js\"];\n  const css_urls = [];\n  \n\n  const inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(null)).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd431fe93b1a4dbc8239385176889dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Load sample metadata:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "b/vo_agam_release/o/v3.7%2Fmetadata%2Fspecies_calls_20200422%2F1246-VO-TZ-KABULA-VMF00185%2Fsamples.species_aim.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/gaard/lib/python3.8/site-packages/malariagen_data/ag3.py:708\u001b[0m, in \u001b[0;36mAg3.sample_metadata\u001b[0;34m(self, sample_sets, sample_query)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 708\u001b[0m     df_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache_sample_metadata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcache_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    711\u001b[0m \n\u001b[1;32m    712\u001b[0m     \u001b[38;5;66;03m# concatenate multiple sample sets\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: ('1246-VO-TZ-KABULA-VMF00185',)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/gaard/lib/python3.8/site-packages/malariagen_data/ag3.py:516\u001b[0m, in \u001b[0;36mAg3._read_species_calls\u001b[0;34m(self, sample_set)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 516\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache_species_calls\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: '1246-VO-TZ-KABULA-VMF00185'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m ag3 \u001b[38;5;241m=\u001b[39m malariagen_data\u001b[38;5;241m.\u001b[39mAg3(pre\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m sample_sets \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1246-VO-TZ-KABULA-VMF00185\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m meta \u001b[38;5;241m=\u001b[39m \u001b[43mag3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_sets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msample_sets\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m meta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m meta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_.*\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Make sure the samples are in the same order as the CNV data (they should be anyway, but \u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# just in case)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gaard/lib/python3.8/site-packages/malariagen_data/ag3.py:719\u001b[0m, in \u001b[0;36mAg3.sample_metadata\u001b[0;34m(self, sample_sets, sample_query)\u001b[0m\n\u001b[1;32m    715\u001b[0m sample_sets_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_progress(\n\u001b[1;32m    716\u001b[0m     sample_sets, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoad sample metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    717\u001b[0m )\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sample_sets_iterator:\n\u001b[0;32m--> 719\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m    721\u001b[0m df_samples \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dfs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gaard/lib/python3.8/site-packages/malariagen_data/ag3.py:673\u001b[0m, in \u001b[0;36mAg3._sample_metadata\u001b[0;34m(self, sample_set)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sample_metadata\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, sample_set):\n\u001b[1;32m    672\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_general_metadata(sample_set\u001b[38;5;241m=\u001b[39msample_set)\n\u001b[0;32m--> 673\u001b[0m     df_species \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_species_calls\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mmerge(df_species, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    675\u001b[0m     df_cohorts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_cohort_metadata(sample_set\u001b[38;5;241m=\u001b[39msample_set)\n",
      "File \u001b[0;32m~/miniconda3/envs/gaard/lib/python3.8/site-packages/malariagen_data/ag3.py:530\u001b[0m, in \u001b[0;36mAg3._read_species_calls\u001b[0;34m(self, sample_set)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    528\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown species calling analysis: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_species_analysis\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    529\u001b[0m     )\n\u001b[0;32m--> 530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    531\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[1;32m    532\u001b[0m         f,\n\u001b[1;32m    533\u001b[0m         na_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    538\u001b[0m         },\n\u001b[1;32m    539\u001b[0m     )\n\u001b[1;32m    541\u001b[0m \u001b[38;5;66;03m# add a single species call column, for convenience\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gaard/lib/python3.8/site-packages/fsspec/spec.py:1009\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1008\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[0;32m-> 1009\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gcsfs/core.py:1205\u001b[0m, in \u001b[0;36mGCSFileSystem._open\u001b[0;34m(self, path, mode, block_size, cache_options, acl, consistency, metadata, autocommit, fixed_key_metadata, **kwargs)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     block_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_block_size\n\u001b[1;32m   1204\u001b[0m const \u001b[38;5;241m=\u001b[39m consistency \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconsistency\n\u001b[0;32m-> 1205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGCSFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconsistency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m    \u001b[49m\u001b[43macl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautocommit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_key_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_key_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gcsfs/core.py:1325\u001b[0m, in \u001b[0;36mGCSFile.__init__\u001b[0;34m(self, gcsfs, path, mode, block_size, autocommit, cache_type, cache_options, acl, consistency, metadata, content_type, timeout, fixed_key_metadata, **kwargs)\u001b[0m\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1272\u001b[0m     gcsfs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1285\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1286\u001b[0m ):\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;124;03m    Open a file.\u001b[39;00m\n\u001b[1;32m   1289\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;124;03m        Timeout seconds for the asynchronous callback.\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgcsfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautocommit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1335\u001b[0m     bucket, key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39msplit_path(path)\n\u001b[1;32m   1336\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m key:\n",
      "File \u001b[0;32m~/miniconda3/envs/gaard/lib/python3.8/site-packages/fsspec/spec.py:1351\u001b[0m, in \u001b[0;36mAbstractBufferedFile.__init__\u001b[0;34m(self, fs, path, mode, block_size, autocommit, cache_type, cache_options, size, **kwargs)\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1351\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetails\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1352\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;241m=\u001b[39m caches[cache_type](\n\u001b[1;32m   1353\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_range, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcache_options\n\u001b[1;32m   1354\u001b[0m     )\n\u001b[1;32m   1355\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/gaard/lib/python3.8/site-packages/fsspec/spec.py:1364\u001b[0m, in \u001b[0;36mAbstractBufferedFile.details\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetails\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1363\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_details \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1364\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_details \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_details\n",
      "File \u001b[0;32m~/miniconda3/envs/gaard/lib/python3.8/site-packages/fsspec/asyn.py:91\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gaard/lib/python3.8/site-packages/fsspec/asyn.py:71\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FSTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreturn_result\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m return_result\n",
      "File \u001b[0;32m~/miniconda3/envs/gaard/lib/python3.8/site-packages/fsspec/asyn.py:25\u001b[0m, in \u001b[0;36m_runner\u001b[0;34m(event, coro, result, timeout)\u001b[0m\n\u001b[1;32m     23\u001b[0m     coro \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mwait_for(coro, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     27\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m ex\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gcsfs/core.py:726\u001b[0m, in \u001b[0;36mGCSFileSystem._info\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    725\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetail\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Force to true for info\u001b[39;00m\n\u001b[0;32m--> 726\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ls(path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    727\u001b[0m out0 \u001b[38;5;241m=\u001b[39m [o \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m out \u001b[38;5;28;01mif\u001b[39;00m o[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m path]\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out0:\n\u001b[1;32m    729\u001b[0m     \u001b[38;5;66;03m# exact hit\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gcsfs/core.py:762\u001b[0m, in \u001b[0;36mGCSFileSystem._ls\u001b[0;34m(self, path, detail, prefix, **kwargs)\u001b[0m\n\u001b[1;32m    760\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_list_buckets()\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 762\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_list_objects(path, prefix\u001b[38;5;241m=\u001b[39mprefix)\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m detail:\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gcsfs/core.py:504\u001b[0m, in \u001b[0;36mGCSFileSystem._list_objects\u001b[0;34m(self, path, prefix)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (items \u001b[38;5;241m+\u001b[39m pseudodirs):\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key:\n\u001b[0;32m--> 504\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object(path)]\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gcsfs/core.py:458\u001b[0m, in \u001b[0;36mGCSFileSystem._get_object\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m# Work around various permission settings. Prefer an object get (storage.objects.get), but\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;66;03m# fall back to a bucket list + filter to object name (storage.objects.list).\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 458\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/o/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, bucket, key, json_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForbidden\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gcsfs/core.py:392\u001b[0m, in \u001b[0;36mGCSFileSystem._call\u001b[0;34m(self, method, path, json_out, info_out, *args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m, method, path, \u001b[38;5;241m*\u001b[39margs, json_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, info_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    390\u001b[0m ):\n\u001b[1;32m    391\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 392\u001b[0m     status, headers, info, contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    393\u001b[0m         method, path, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    394\u001b[0m     )\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m json_out:\n\u001b[1;32m    396\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(contents)\n",
      "File \u001b[0;32m~/miniconda3/envs/gaard/lib/python3.8/site-packages/decorator.py:221\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    220\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gcsfs/retry.py:115\u001b[0m, in \u001b[0;36mretry_request\u001b[0;34m(func, retries, *args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retry \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mmin\u001b[39m(random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (retry \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m32\u001b[39m))\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    117\u001b[0m     HttpError,\n\u001b[1;32m    118\u001b[0m     requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m     aiohttp\u001b[38;5;241m.\u001b[39mclient_exceptions\u001b[38;5;241m.\u001b[39mClientError,\n\u001b[1;32m    122\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(e, HttpError)\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m e\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequester pays\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39mmessage\n\u001b[1;32m    127\u001b[0m     ):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gcsfs/core.py:385\u001b[0m, in \u001b[0;36mGCSFileSystem._request\u001b[0;34m(self, method, path, headers, json, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m info \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mrequest_info  \u001b[38;5;66;03m# for debug only\u001b[39;00m\n\u001b[1;32m    383\u001b[0m contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m r\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 385\u001b[0m \u001b[43mvalidate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m status, headers, info, contents\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gcsfs/retry.py:84\u001b[0m, in \u001b[0;36mvalidate_response\u001b[0;34m(status, content, path, args)\u001b[0m\n\u001b[1;32m     82\u001b[0m     path \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m[quote(p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m args])\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(path)\n\u001b[1;32m     86\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(content, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecode\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: b/vo_agam_release/o/v3.7%2Fmetadata%2Fspecies_calls_20200422%2F1246-VO-TZ-KABULA-VMF00185%2Fsamples.species_aim.csv"
     ]
    }
   ],
   "source": [
    "ag3 = malariagen_data.Ag3(pre=True)\n",
    "\n",
    "sample_sets = ['1246-VO-TZ-KABULA-VMF00185']\n",
    "meta = ag3.sample_metadata(sample_sets = sample_sets).set_index('sample_id')\n",
    "meta['location'] = meta['location'].str.replace('_.*', '', regex=True)\n",
    "\n",
    "# Make sure the samples are in the same order as the CNV data (they should be anyway, but \n",
    "# just in case)\n",
    "meta = meta.loc[discordant_read_calls.index, :]\n",
    "\n",
    "# Get haplotype-level metadata\n",
    "hap_meta = pd.DataFrame(np.repeat(meta.values, 2, axis = 0))\n",
    "hap_meta.columns = meta.columns\n",
    "hap_meta.index = [np.repeat(meta.index, 2)[i] + ['a', 'b'][i%2] for i in range(len(hap_meta))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hap_meta_Dup31 = hap_meta.loc[~np.repeat(exclude_samples_Dup31, 2), :]\n",
    "hap_meta_Dup33 = hap_meta.loc[~np.repeat(exclude_samples_Dup33, 2), :]\n",
    "hap_meta_Dup31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_copy_number = pd.read_csv('../Ag1000G_CNV_data/v3.7_1246-VO-TZ-KABULA-VMF00185/modal_CNVs/modal_copy_number_arabiensis.csv', sep = '\\t', index_col = 0)\n",
    "\n",
    "# Define genes of interest\n",
    "cyp6_genes = dict(AGAP002862='Cyp6aa1')\n",
    "\n",
    "# Select data for genes of interest, excluding samples with poor quality HMM data\n",
    "cyp6_gene_copy_number = (\n",
    "    gene_copy_number.loc[:, cyp6_genes.keys()]\n",
    "    .rename(cyp6_genes, axis='columns')\n",
    ")['Cyp6aa1'].values\n",
    "\n",
    "Dup31_copy_number = cyp6_gene_copy_number[~exclude_samples_Dup31] * Dup31_calls[~exclude_samples_Dup31]\n",
    "Dup33_copy_number = cyp6_gene_copy_number[~exclude_samples_Dup33] * Dup33_calls[~exclude_samples_Dup33]\n",
    "\n",
    "display(pd.crosstab(meta.loc[~exclude_samples_Dup31, 'location'], Dup31_copy_number))\n",
    "display(pd.crosstab(meta.loc[~exclude_samples_Dup33, 'location'], Dup33_copy_number))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly all samples have either 0, 1 or 2 extra copies.\n",
    "\n",
    "Can we assume that we just have a simple duplication that is either het or hom?\n",
    "\n",
    "Let's check Hardy_weinberg equilibrium under that assumption:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! echo \"library('HardyWeinberg'); HWExact(c(181, 19, 13))\" | R --vanilla 2>/dev/null | tail -n7\n",
    "! echo -e \"\\n\"\n",
    "! echo \"library('HardyWeinberg'); HWExact(c(66, 60, 74))\" | R --vanilla 2>/dev/null | tail -n7\n",
    "! echo -e \"\\n\"\n",
    "! echo \"library('HardyWeinberg'); HWExact(c(167, 13, 0))\" | R --vanilla 2>/dev/null | tail -n7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's highly significantly divergent from expectations in Moshi. So in Muleba, it looks like Dup33 is single extra copy, but in Moshi it looks like both dups have a deficit of heterozygotes, suggesting that perhaps there is a two-copy version present as well. Other explanations are mis-calls (there definitely are occasional mistakes in copy-number estimation) and the fact that this is a resistance locus, and the nature of the GAARD bioassay protocol means that there will be a deficit of intermediate phenotypes, which might be CNV heterozygotes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the haplotype data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region='2R:28250000-28750000'\n",
    "haps = (\n",
    "    ag3.haplotypes(region = region, \n",
    "                   analysis = 'arab', \n",
    "                   sample_sets = sample_sets)\n",
    "    .set_index(samples = 'sample_id')\n",
    "    .sel(samples = discordant_read_calls.index)\n",
    ")\n",
    "\n",
    "haps_Dup31 = haps.isel(samples = ~exclude_samples_Dup31)\n",
    "haps_Dup33 = haps.isel(samples = ~exclude_samples_Dup33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create haplotype array\n",
    "hap_array_Dup31 = allel.GenotypeArray(haps_Dup31.call_genotype).to_haplotypes()\n",
    "\n",
    "# Get allele counts\n",
    "ac_Dup31 = hap_array_Dup31.count_alleles()\n",
    "\n",
    "# Identify segregating non-singletons. \n",
    "non_singleton_Dup31 = ac_Dup31.min(1) > 1\n",
    "\n",
    "# Filter the datasets, removing non-segregating and singleton variants \n",
    "haps_Dup31 = haps_Dup31.isel(variants = np.where(non_singleton_Dup31)[0])\n",
    "hap_array_Dup31 = hap_array_Dup31[non_singleton_Dup31]\n",
    "\n",
    "# Same again for Dup_33\n",
    "hap_array_Dup33 = allel.GenotypeArray(haps_Dup33.call_genotype).to_haplotypes()\n",
    "ac_Dup33 = hap_array_Dup33.count_alleles()\n",
    "non_singleton_Dup33 = ac_Dup33.min(1) > 1\n",
    "haps_Dup33 = haps_Dup33.isel(variants = np.where(non_singleton_Dup33)[0])\n",
    "hap_array_Dup33 = hap_array_Dup33[non_singleton_Dup33]\n",
    "\n",
    "hap_array_Dup31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the SNP genotypes coded as 0, 1, 2 (wt, het, homozygote mutant). Since all phased SNPs\n",
    "# are biallelic, this is a suitable way to code genotype. \n",
    "genotypes_Dup31 = haps_Dup31.call_genotype.values.sum(2)\n",
    "genotypes_Dup33 = haps_Dup33.call_genotype.values.sum(2)\n",
    "\n",
    "genotypes_Dup31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify proxy SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look for SNPs whose presence / absence patterns are perfectly correlated with Dup31. \n",
    "SNP_cor_Dup31 = np.apply_along_axis(lambda x: spearmanr((x>0).astype('int'), \n",
    "                                                         Dup31_calls[~exclude_samples_Dup31]), \n",
    "                                    1, \n",
    "                                    genotypes_Dup31)[:, 0]\n",
    "print(f'The maximum correlation coefficient with Dup31 is {np.nanmax(SNP_cor_Dup31)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look for SNPs whose presence / absence patterns are perfectly correlated with Dup33. \n",
    "SNP_cor_Dup33 = np.apply_along_axis(lambda x: spearmanr((x>0).astype('int'), \n",
    "                                                         Dup33_calls[~exclude_samples_Dup33]), \n",
    "                                    1, \n",
    "                                    genotypes_Dup33)[:, 0]\n",
    "print(f'The maximum correlation coefficient with Dup33 is {np.nanmax(SNP_cor_Dup33)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's look at the association of zygosity at those SNPs and Dup31 copy number. \n",
    "Dup31_proxy = genotypes_Dup31[np.nanargmax(SNP_cor_Dup31)]\n",
    "\n",
    "print('Association of Dup31 proxy zygosity and Dup31 copy number in Moshi:')\n",
    "display(\n",
    "    pd.crosstab(Dup31_copy_number[(meta.loc[~exclude_samples_Dup31, :].location == 'Moshi').values],\n",
    "                Dup31_proxy[(meta.loc[~exclude_samples_Dup31, :].location == 'Moshi').values])\n",
    ")\n",
    "\n",
    "print('Association of Dup31 proxy zygosity and Dup31 copy number in Muleba:')\n",
    "display(\n",
    "    pd.crosstab(Dup31_copy_number[(meta.loc[~exclude_samples_Dup31, :].location == 'Muleba').values],\n",
    "                Dup31_proxy[(meta.loc[~exclude_samples_Dup31, :].location == 'Muleba').values])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the association of zygosity at those SNPs and Dup33 copy number. \n",
    "Dup33_proxy = genotypes_Dup33[np.nanargmax(SNP_cor_Dup33)]\n",
    "\n",
    "print('Association of Dup33 proxy zygosity and Dup33 copy number in Moshi:')\n",
    "display(\n",
    "    pd.crosstab(Dup33_copy_number[(meta.loc[~exclude_samples_Dup33, :].location == 'Moshi').values],\n",
    "                Dup33_proxy[(meta.loc[~exclude_samples_Dup33, :].location == 'Moshi').values])\n",
    ")\n",
    "\n",
    "print('Association of Dup33 proxy zygosity and Dup33 copy number in Muleba:')\n",
    "display(\n",
    "    pd.crosstab(Dup33_copy_number[(meta.loc[~exclude_samples_Dup33, :].location == 'Muleba').values],\n",
    "                Dup33_proxy[(meta.loc[~exclude_samples_Dup33, :].location == 'Muleba').values])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the proxy for Dup31 is useless, but for Dup33 it's fairly good.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the haplotypes rather than genotypes for the Dup proxies\n",
    "Dup31_hap_proxy = hap_array_Dup31[np.nanargmax(SNP_cor_Dup31)]\n",
    "Dup33_hap_proxy = hap_array_Dup33[np.nanargmax(SNP_cor_Dup33)]\n",
    "Dup31_hap_proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Haplotype plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_clusters(dist, n, threshold=0.001, method='complete'):\n",
    "        # build hierarchy\n",
    "        clust = scipy.cluster.hierarchy.linkage(dist, method=method)\n",
    "        # find clusters\n",
    "        f = scipy.cluster.hierarchy.fcluster(clust, threshold,\n",
    "                                             criterion='distance')\n",
    "        # compute cluster sizes\n",
    "        fsz = np.bincount(f)\n",
    "        # sort largest first\n",
    "        fsort = np.argsort(fsz)[::-1]\n",
    "        # take largest n\n",
    "        fsort = fsort[:n]\n",
    "        # get haplotype indices for each cluster\n",
    "        clusters = [set(np.nonzero(f == i)[0]) for i in fsort]\n",
    "        return clusters\n",
    "\n",
    "# Some plotting functions\n",
    "def truspan(cluster, r):\n",
    "    # get the index of the cluster haps in the dendrogram list of all haps\n",
    "    cluster_leaves = sorted([r['leaves'].index(i) for i in cluster])\n",
    "    # are these indices monotonic - they should be!\n",
    "    x = np.asarray(cluster_leaves)\n",
    "    dx = np.diff(x)\n",
    "    mon = np.all(dx == 1)\n",
    "    assert mon\n",
    "    return min(cluster_leaves), max(cluster_leaves)\n",
    "\n",
    "def plot_dendrogram(zhier, ax, h, method='complete', color_threshold=0, above_threshold_color='k'):\n",
    "\n",
    "    # plot dendrogram\n",
    "    sns.despine(ax=ax, offset=5, bottom=True, top=False)\n",
    "    r = scipy.cluster.hierarchy.dendrogram(zhier, no_labels=True, count_sort=True,\n",
    "                                           color_threshold=color_threshold,\n",
    "                                           above_threshold_color=above_threshold_color,\n",
    "                                           ax=ax)\n",
    "    xmin, xmax = ax.xaxis.get_data_interval()\n",
    "    xticklabels = np.array(list(range(0, h.shape[1], 200)) + [h.shape[1]])\n",
    "    xticks = xticklabels / h.shape[1]\n",
    "    xticks = (xticks * (xmax - xmin)) + xmin\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_xticklabels(xticklabels)\n",
    "    #ax.set_xlabel('Haplotypes')\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    ax.set_ylim(bottom=-.0001)\n",
    "\n",
    "    ax.set_ylabel(r'$d_{xy}$')\n",
    "    ax.autoscale(axis='x', tight=True)\n",
    "    return r\n",
    "\n",
    "def draw_hap_cluster_plot(z, r, h, \n",
    "                          pop_colour_scheme, cnv_colour_scheme, \n",
    "                          labels_for_pop_colours, labels_for_cnv_colours, \n",
    "                          cnv_name,\n",
    "                          add_legend = True, title = '',\n",
    "                          fn = None):\n",
    "\n",
    "    gs = GridSpec(3, 1, height_ratios=[6.0, 0.5, 0.5])\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.02)\n",
    "    ax1 = plt.subplot(gs[0])\n",
    "    sns.despine(ax=ax1, offset=5, bottom=True, top=True)\n",
    "    _ = plot_dendrogram(z, ax1, h)\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_ylim(0)\n",
    "\n",
    "    ax_pops = fig.add_subplot(gs[1])\n",
    "\n",
    "    x_pop = labels_for_pop_colours.take(r['leaves'])\n",
    "    hap_pop_clrs = [pop_colour_scheme[p] for p in x_pop]\n",
    "    ax_pops.broken_barh(xranges=[(i, 1) for i in range(h.shape[1])],\n",
    "                        yrange=(0, 1),\n",
    "                        color=hap_pop_clrs);\n",
    "    sns.despine(ax=ax_pops, offset=5, left=True, bottom=True)\n",
    "    \n",
    "    ax_pops.set_xticks([])\n",
    "    ax_pops.set_yticks([])\n",
    "    ax_pops.set_xlim(0, h.shape[1])\n",
    "    ax_pops.yaxis.set_label_position('left')\n",
    "    ax_pops.set_ylabel('Population', rotation=0, ha='right', va='center')\n",
    "    \n",
    "    ax_cnv = fig.add_subplot(gs[2])\n",
    "\n",
    "    x_cnv = labels_for_cnv_colours.take(r['leaves'])\n",
    "    hap_cnv_clrs = [cnv_colour_scheme[p] for p in x_cnv]\n",
    "    ax_cnv.broken_barh(xranges=[(i, 1) for i in range(h.shape[1])],\n",
    "                       yrange=(0, 1),\n",
    "                       color=hap_cnv_clrs);\n",
    "    sns.despine(ax=ax_cnv, offset=5, left=True, bottom=True, top=True)\n",
    "\n",
    "    ax_cnv.set_xticks([])\n",
    "    ax_cnv.set_yticks([])\n",
    "    ax_cnv.set_xlim(0, h.shape[1])\n",
    "    ax_cnv.yaxis.set_label_position('left')\n",
    "    ax_cnv.set_ylabel(cnv_name, rotation=0, ha='right', va='center')\n",
    "\n",
    "    if add_legend:\n",
    "        plot_x_range = ax1.get_xlim()[1] - ax1.get_xlim()[0]\n",
    "        plot_y_range = ax1.get_ylim()[1] - ax1.get_ylim()[0]\n",
    "        legend_x = ax1.get_xlim()[0] + plot_x_range * 0.95\n",
    "            \n",
    "        unique_label_cnv = np.unique(x_cnv)\n",
    "        for i, k in enumerate(unique_label_cnv[::-1]):\n",
    "            legend_y = ax1.get_ylim()[0] + plot_y_range * (0.2 + 0.1 * i)\n",
    "            ax1.add_patch(mpl.patches.Rectangle((legend_x,legend_y), plot_x_range/50,plot_y_range/15, color = cnv_colour_scheme[k]))\n",
    "            ax1.text(legend_x*1.03,legend_y + plot_y_range / 50, k)\n",
    "\n",
    "        \n",
    "        unique_label_pop = np.unique(x_pop)\n",
    "        for j, k in enumerate(unique_label_pop[::-1]):\n",
    "            legend_y = ax1.get_ylim()[0] + plot_y_range * (0.2 + 0.1 * (i + 2 + j))\n",
    "            ax1.add_patch(mpl.patches.Rectangle((legend_x,legend_y), plot_x_range/50,plot_y_range/15, color = pop_colour_scheme[k]))\n",
    "            ax1.text(legend_x*1.03,legend_y + plot_y_range / 50, k)\n",
    "\n",
    "\n",
    "\n",
    "    ax1.set_title(f'{title}')\n",
    "    ax1.autoscale()\n",
    "    fig.patch.set_alpha(1)\n",
    "    \n",
    "    if fn != None:\n",
    "        ext = re.sub('.*\\.', '', fn)\n",
    "        plt.savefig(fn, format = ext)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Haplotype clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Home in on the 500 SNPs around the middle of Cyp6aa1\n",
    "focal_pos = 28481606 # Middle of Cyp6aa1\n",
    "window_size = 500  # Measured in SNPs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_snp_index_Dup31 = np.argmin(np.abs(haps_Dup31.variant_position - focal_pos).values)\n",
    "haptree_region_indices_Dup31 = range(nearest_snp_index_Dup31 - window_size//2, nearest_snp_index_Dup31 + window_size//2)\n",
    "haptree_region_Dup31 = '2R:' + '-'.join(haps_Dup31.isel(variants = haptree_region_indices_Dup31).variant_position.values[[0,-1]].astype('str'))\n",
    "loc_hap_Dup31 = hap_array_Dup31[haptree_region_indices_Dup31]\n",
    "# loc_hap is the haplotype array for just those 500 SNPs. \n",
    "\n",
    "# Calculate the distance matrix \n",
    "dist = allel.pairwise_distance(loc_hap_Dup31, metric = 'hamming')\n",
    "site_filter = ag3.snp_calls(region=haptree_region_Dup31, sample_sets=\"3.7\")['variant_filter_pass_arab']\n",
    "n_bases = np.sum(site_filter.values)\n",
    "dist_dxy_Dup31 = dist * loc_hap_Dup31.n_variants / n_bases\n",
    "\n",
    "z = scipy.cluster.hierarchy.linkage(dist_dxy_Dup31, method=\"complete\")\n",
    "r = scipy.cluster.hierarchy.dendrogram(\n",
    "        z, no_labels=True, count_sort=True,\n",
    "        color_threshold=0, no_plot=True,\n",
    "        above_threshold_color='k')\n",
    "\n",
    "# Now let's plot the haplotype clustering tree\n",
    "Dup31_status = np.array(['No CNV', '1 extra copy', '2 extra copies'])[np.repeat(Dup31_copy_number, 2)]\n",
    "draw_hap_cluster_plot(z, r, loc_hap_Dup31, \n",
    "                      pop_colour_scheme = pop_colours, \n",
    "                      cnv_colour_scheme = dup_colours,\n",
    "                      labels_for_pop_colours = hap_meta_Dup31['location'], \n",
    "                      labels_for_cnv_colours = Dup31_status, \n",
    "                      cnv_name = 'Cyp6aap_Dup31',\n",
    "                      title = 'Haplotypes around Cyp6',\n",
    "                      fn = 'Cyp6_Dup31_haplotype_clustering.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_snp_index_Dup33 = np.argmin(np.abs(haps_Dup33.variant_position - focal_pos).values)\n",
    "haptree_region_indices_Dup33 = range(nearest_snp_index_Dup33 - window_size//2, nearest_snp_index_Dup33 + window_size//2)\n",
    "haptree_region_Dup33 = '2R:' + '-'.join(haps_Dup33.isel(variants = haptree_region_indices_Dup33).variant_position.values[[0,-1]].astype('str'))\n",
    "loc_hap_Dup33 = hap_array_Dup33[haptree_region_indices_Dup33]\n",
    "# loc_hap is the haplotype array for just those 500 SNPs. \n",
    "\n",
    "# Calculate the distance matrix \n",
    "dist = allel.pairwise_distance(loc_hap_Dup33, metric = 'hamming')\n",
    "site_filter = ag3.snp_calls(region=haptree_region_Dup33, sample_sets=\"3.7\")['variant_filter_pass_arab']\n",
    "n_bases = np.sum(site_filter.values)\n",
    "dist_dxy_Dup33 = dist * loc_hap_Dup33.n_variants / n_bases\n",
    "\n",
    "z = scipy.cluster.hierarchy.linkage(dist_dxy_Dup33, method=\"complete\")\n",
    "r = scipy.cluster.hierarchy.dendrogram(\n",
    "        z, no_labels=True, count_sort=True,\n",
    "        color_threshold=0, no_plot=True,\n",
    "        above_threshold_color='k')\n",
    "\n",
    "# Now let's plot the haplotype clustering tree\n",
    "Dup33_hap_status = np.array(['Dup33 absent', 'Dup33 present'])[Dup33_hap_proxy]\n",
    "draw_hap_cluster_plot(z, r, loc_hap_Dup33, \n",
    "                      pop_colour_scheme = pop_colours, \n",
    "                      cnv_colour_scheme = dup_colours,\n",
    "                      labels_for_pop_colours = hap_meta_Dup33['location'], \n",
    "                      labels_for_cnv_colours = Dup33_hap_status, \n",
    "                      cnv_name = 'Cyp6aap_Dup33',\n",
    "                      title = 'Haplotypes around Cyp6',\n",
    "                      fn = 'Cyp6_Dup33_haplotype_clustering.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of labelling by region, label by phenotype\n",
    "phen = pd.read_csv('../../data/sample_phenotypes_EA.csv', sep = '\\t', index_col = 'specimen')\n",
    "phen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hap_meta_Dup31['insecticide'] = phen.loc[hap_meta_Dup31['partner_sample_id'], ['insecticide']].values\n",
    "hap_meta_Dup31['phenotype'] = phen.loc[hap_meta_Dup31['partner_sample_id'], ['phenotype']].values\n",
    "hap_meta_Dup33['insecticide'] = phen.loc[hap_meta_Dup33['partner_sample_id'], ['insecticide']].values\n",
    "hap_meta_Dup33['phenotype'] = phen.loc[hap_meta_Dup33['partner_sample_id'], ['phenotype']].values\n",
    "hap_meta_Dup33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_delta_Dup31 = hap_meta_Dup31['insecticide'] == 'Delta'\n",
    "which_delta_Dup33 = hap_meta_Dup33['insecticide'] == 'Delta'\n",
    "hap_meta_Dup31_delta = hap_meta_Dup31.loc[which_delta_Dup31, :]\n",
    "hap_meta_Dup33_delta = hap_meta_Dup33.loc[which_delta_Dup33, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phen_colours = {\n",
    " 'dead' : \"red\",\n",
    " 'alive' : \"blue\",\n",
    "}\n",
    "\n",
    "# Calculate the distance matrix \n",
    "loc_hap_Dup31_delta = loc_hap_Dup31[:, which_delta_Dup31]\n",
    "dist = allel.pairwise_distance(loc_hap_Dup31_delta, metric = 'hamming')\n",
    "site_filter = ag3.snp_calls(region=haptree_region_Dup31, sample_sets=\"3.7\")['variant_filter_pass_arab']\n",
    "n_bases = np.sum(site_filter.values)\n",
    "dist_dxy_Dup31 = dist * loc_hap_Dup31_delta.n_variants / n_bases\n",
    "\n",
    "z = scipy.cluster.hierarchy.linkage(dist_dxy_Dup31, method=\"complete\")\n",
    "r = scipy.cluster.hierarchy.dendrogram(\n",
    "        z, no_labels=True, count_sort=True,\n",
    "        color_threshold=0, no_plot=True,\n",
    "        above_threshold_color='k')\n",
    "\n",
    "# Now let's plot the haplotype clustering tree\n",
    "Dup31_status = np.array(['No CNV', '1 extra copy', '2 extra copies'])[np.repeat(Dup31_copy_number, 2)[which_delta_Dup31]]\n",
    "draw_hap_cluster_plot(z, r, loc_hap_Dup31, \n",
    "                      pop_colour_scheme = phen_colours, \n",
    "                      cnv_colour_scheme = dup_colours,\n",
    "                      labels_for_pop_colours = hap_meta_Dup31_delta['phenotype'], \n",
    "                      labels_for_cnv_colours = Dup31_status, \n",
    "                      cnv_name = 'Cyp6aap_Dup31',\n",
    "                      title = 'Haplotypes around Cyp6',\n",
    "                      fn = 'Cyp6_Dup31_haplotype_clustering_phenotype.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the distance matrix \n",
    "loc_hap_Dup33_delta = loc_hap_Dup33[:, which_delta_Dup33]\n",
    "dist = allel.pairwise_distance(loc_hap_Dup33_delta, metric = 'hamming')\n",
    "site_filter = ag3.snp_calls(region=haptree_region_Dup33, sample_sets=\"3.7\")['variant_filter_pass_arab']\n",
    "n_bases = np.sum(site_filter.values)\n",
    "dist_dxy_Dup33 = dist * loc_hap_Dup33_delta.n_variants / n_bases\n",
    "\n",
    "z = scipy.cluster.hierarchy.linkage(dist_dxy_Dup33, method=\"complete\")\n",
    "r = scipy.cluster.hierarchy.dendrogram(\n",
    "        z, no_labels=True, count_sort=True,\n",
    "        color_threshold=0, no_plot=True,\n",
    "        above_threshold_color='k')\n",
    "\n",
    "# Now let's plot the haplotype clustering tree\n",
    "Dup33_hap_status = np.array(['Dup33 absent', 'Dup33 present'])[Dup33_hap_proxy[which_delta_Dup33]]\n",
    "draw_hap_cluster_plot(z, r, loc_hap_Dup33_delta, \n",
    "                      pop_colour_scheme = phen_colours, \n",
    "                      cnv_colour_scheme = dup_colours,\n",
    "                      labels_for_pop_colours = hap_meta_Dup33_delta['phenotype'], \n",
    "                      labels_for_cnv_colours = Dup33_hap_status, \n",
    "                      cnv_name = 'Cyp6aap_Dup33',\n",
    "                      title = 'Haplotypes around Cyp6',\n",
    "                      fn = 'Cyp6_Dup33_haplotype_clustering_phenotype.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_hap_cluster_plot(z, r, h, \n",
    "                          pop_colour_scheme, cnv1_colour_scheme, cnv2_colour_scheme,\n",
    "                          labels_for_pop_colours, labels_for_cnv1_colours, labels_for_cnv2_colours,\n",
    "                          cnv1_name, cnv2_name,\n",
    "                          add_legend = True, title = '',\n",
    "                          fn = None):\n",
    "\n",
    "    gs = GridSpec(3, 1, height_ratios=[6.0, 0.5, 0.5, 0.5])\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.02)\n",
    "    ax1 = plt.subplot(gs[0])\n",
    "    sns.despine(ax=ax1, offset=5, bottom=True, top=True)\n",
    "    _ = plot_dendrogram(z, ax1, h)\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_ylim(0)\n",
    "\n",
    "    ax_pops = fig.add_subplot(gs[1])\n",
    "\n",
    "    x_pop = labels_for_pop_colours.take(r['leaves'])\n",
    "    hap_pop_clrs = [pop_colour_scheme[p] for p in x_pop]\n",
    "    ax_pops.broken_barh(xranges=[(i, 1) for i in range(h.shape[1])],\n",
    "                        yrange=(0, 1),\n",
    "                        color=hap_pop_clrs);\n",
    "    sns.despine(ax=ax_pops, offset=5, left=True, bottom=True)\n",
    "    \n",
    "    ax_pops.set_xticks([])\n",
    "    ax_pops.set_yticks([])\n",
    "    ax_pops.set_xlim(0, h.shape[1])\n",
    "    ax_pops.yaxis.set_label_position('left')\n",
    "    ax_pops.set_ylabel('Population', rotation=0, ha='right', va='center')\n",
    "    \n",
    "    ax_cnv1 = fig.add_subplot(gs[2])\n",
    "\n",
    "    x_cnv1 = labels_for_cnv1_colours.take(r['leaves'])\n",
    "    hap_cnv1_clrs = [cnv1_colour_scheme[p] for p in x_cnv1]\n",
    "    ax_cnv1.broken_barh(xranges=[(i, 1) for i in range(h.shape[1])],\n",
    "                        yrange=(0, 1),\n",
    "                        color=hap_cnv1_clrs);\n",
    "    sns.despine(ax=ax_cnv1, offset=5, left=True, bottom=True, top=True)\n",
    "\n",
    "    ax_cnv1.set_xticks([])\n",
    "    ax_cnv1.set_yticks([])\n",
    "    ax_cnv1.set_xlim(0, h.shape[1])\n",
    "    ax_cnv1.yaxis.set_label_position('left')\n",
    "    ax_cnv1.set_ylabel(cnv1_name, rotation=0, ha='right', va='center')    \n",
    "    ax_cnv1 = fig.add_subplot(gs[2])\n",
    "                                 \n",
    "    ax_cnv2 = fig.add_subplot(gs[3])                             \n",
    "\n",
    "    x_cnv2 = labels_for_cnv2_colours.take(r['leaves'])\n",
    "    hap_cnv2_clrs = [cnv2_colour_scheme[p] for p in x_cnv2]\n",
    "    ax_cnv2.broken_barh(xranges=[(i, 1) for i in range(h.shape[1])],\n",
    "                        yrange=(0, 1),\n",
    "                        color=hap_cnv2_clrs);\n",
    "    sns.despine(ax=ax_cnv2, offset=5, left=True, bottom=True, top=True)\n",
    "\n",
    "    ax_cnv2.set_xticks([])\n",
    "    ax_cnv2.set_yticks([])\n",
    "    ax_cnv2.set_xlim(0, h.shape[1])\n",
    "    ax_cnv2.yaxis.set_label_position('left')\n",
    "    ax_cnv2.set_ylabel(cnv2_name, rotation=0, ha='right', va='center')\n",
    "\n",
    "    if add_legend:\n",
    "        plot_x_range = ax1.get_xlim()[1] - ax1.get_xlim()[0]\n",
    "        plot_y_range = ax1.get_ylim()[1] - ax1.get_ylim()[0]\n",
    "        legend_x = ax1.get_xlim()[0] + plot_x_range * 0.95\n",
    "            \n",
    "        unique_label_cnv2 = np.unique(x_cnv2)\n",
    "        for i, k in enumerate(unique_label_cnv2[::-1]):\n",
    "            legend_y = ax1.get_ylim()[0] + plot_y_range * (0.2 + 0.1 * i)\n",
    "            ax1.add_patch(mpl.patches.Rectangle((legend_x,legend_y), plot_x_range/50,plot_y_range/15, color = cnv2_colour_scheme[k]))\n",
    "            ax1.text(legend_x*1.03,legend_y + plot_y_range / 50, k)\n",
    "\n",
    "        unique_label_cnv1 = np.unique(x_cnv1)\n",
    "        for i, k in enumerate(unique_label_cnv1[::-1]):\n",
    "            legend_y = ax1.get_ylim()[0] + plot_y_range * (0.2 + 0.1 * i)\n",
    "            ax1.add_patch(mpl.patches.Rectangle((legend_x,legend_y), plot_x_range/50,plot_y_range/15, color = cnv1_colour_scheme[k]))\n",
    "            ax1.text(legend_x*1.03,legend_y + plot_y_range / 50, k)\n",
    "        \n",
    "        unique_label_pop = np.unique(x_pop)\n",
    "        for j, k in enumerate(unique_label_pop[::-1]):\n",
    "            legend_y = ax1.get_ylim()[0] + plot_y_range * (0.2 + 0.1 * (i + 2 + j))\n",
    "            ax1.add_patch(mpl.patches.Rectangle((legend_x,legend_y), plot_x_range/50,plot_y_range/15, color = pop_colour_scheme[k]))\n",
    "            ax1.text(legend_x*1.03,legend_y + plot_y_range / 50, k)\n",
    "\n",
    "    ax1.set_title(f'{title}')\n",
    "    ax1.autoscale()\n",
    "    fig.patch.set_alpha(1)\n",
    "    \n",
    "    if fn != None:\n",
    "        ext = re.sub('.*\\.', '', fn)\n",
    "        plt.savefig(fn, format = ext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's plot the haplotype clustering tree\n",
    "Dup33_hap_status = np.array(['Dup33 absent', 'Dup33 present'])[Dup33_hap_proxy]\n",
    "draw_hap_cluster_plot(z, r, loc_hap_Dup33, \n",
    "                      pop_colour_scheme = pop_colours, \n",
    "                      cnv_colour_scheme = dup_colours,\n",
    "                      labels_for_pop_colours = hap_meta_Dup33['location'], \n",
    "                      labels_for_cnv1_colours = Dup31_hap_status, \n",
    "                      labels_for_cnv2_colours = Dup33_hap_status, \n",
    "                      cnv1_name = 'Cyp6aap_Dup31',\n",
    "                      cnv2_name = 'Cyp6aap_Dup33',\n",
    "                      title = 'Haplotypes around Cyp6',\n",
    "                      fn = 'Cyp6_Dup33_haplotype_clustering.png'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaard",
   "language": "python",
   "name": "gaard"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
